"[{\"title\":\"Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning.\",\"authors\":[\"Virat Shejwalkar\",\"Amir Houmansadr\"],\"year\":2021,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"Data Poisoning Attacks to Deep Learning Based Recommender Systems.\",\"authors\":[\"Hai Huang\",\"Jiaming Mu\",\"Neil Zhenqiang Gong\",\"Qi Li\",\"Bin Liu\",\"Mingwei Xu\"],\"year\":2021,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection.\",\"authors\":[\"Phillip Rieger\",\"Thien Duc Nguyen\",\"Markus Miettinen\",\"Ahmad-Reza Sadeghi\"],\"year\":2022,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks.\",\"authors\":[\"Xueluan Gong\",\"Yanjiao Chen\",\"Jianshuo Dong\",\"Qian Wang\"],\"year\":2022,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"Backdoor Attacks Against Dataset Distillation.\",\"authors\":[\"Yugeng Liu\",\"Zheng Li\",\"Michael Backes\",\"Yun Shen\",\"Yang Zhang\"],\"year\":2023,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"Securing Federated Sensitive Topic Classification against Poisoning Attacks.\",\"authors\":[\"Tianyue Chu\",\"\\u00c1lvaro Garc\\u00eda-Recuero\",\"Costas Iordanou\",\"Georgios Smaragdakis\",\"Nikolaos Laoutaris\"],\"year\":2023,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense.\",\"authors\":[\"Siyuan Cheng\",\"Guanhong Tao\",\"Yingqi Liu\",\"Shengwei An\",\"Xiangzhe Xu\",\"Shiwei Feng\",\"Guangyu Shen\",\"Kaiyuan Zhang\",\"Qiuling Xu\",\"Shiqing Ma\",\"Xiangyu Zhang\"],\"year\":2023,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"Automatic Adversarial Adaption for Stealthy Poisoning Attacks in Federated Learning.\",\"authors\":[\"Torsten Krau\\u00df\",\"Jan K\\u00f6nig\",\"Alexandra Dmitrienko\",\"Christian Kanzow\"],\"year\":2024,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning.\",\"authors\":[\"Hossein Fereidooni\",\"Alessandro Pegoraro\",\"Phillip Rieger\",\"Alexandra Dmitrienko\",\"Ahmad-Reza Sadeghi\"],\"year\":2024,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering.\",\"authors\":[\"Rui Zhu\",\"Di Tang\",\"Siyuan Tang\",\"Zihao Wang\",\"Guanhong Tao\",\"Shiqing Ma\",\"XiaoFeng Wang\",\"Haixu Tang\"],\"year\":2024,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data.\",\"authors\":[\"Gorka Abad\",\"Oguzhan Ersoy\",\"Stjepan Picek\",\"Aitor Urbieta\"],\"year\":2024,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"TextGuard: Provable Defense against Backdoor Attacks on Text Classification.\",\"authors\":[\"Hengzhi Pei\",\"Jinyuan Jia\",\"Wenbo Guo\",\"Bo Li\",\"Dawn Song\"],\"year\":2024,\"proceedings\":\"ndss\",\"type\":\"attack\"},{\"title\":\"When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks.\",\"authors\":[\"Octavian Suciu\",\"Radu Marginean\",\"Yigitcan Kaya\",\"Hal Daum\\u00e9 III\",\"Tudor Dumitras\"],\"year\":2018,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks.\",\"authors\":[\"Ambra Demontis\",\"Marco Melis\",\"Maura Pintor\",\"Matthew Jagielski\",\"Battista Biggio\",\"Alina Oprea\",\"Cristina Nita-Rotaru\",\"Fabio Roli\"],\"year\":2019,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Poison Over Troubled Forwarders: A Cache Poisoning Attack Targeting DNS Forwarding Devices.\",\"authors\":[\"Xiaofeng Zheng\",\"Chaoyi Lu\",\"Jian Peng\",\"Qiushi Yang\",\"Dongjie Zhou\",\"Baojun Liu\",\"Keyu Man\",\"Shuang Hao\",\"Haixin Duan\",\"Zhiyun Qian\"],\"year\":2020,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Local Model Poisoning Attacks to Byzantine-Robust Federated Learning.\",\"authors\":[\"Minghong Fang\",\"Xiaoyu Cao\",\"Jinyuan Jia\",\"Neil Zhenqiang Gong\"],\"year\":2020,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Data Poisoning Attacks to Local Differential Privacy Protocols.\",\"authors\":[\"Xiaoyu Cao\",\"Jinyuan Jia\",\"Neil Zhenqiang Gong\"],\"year\":2021,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers.\",\"authors\":[\"Giorgio Severi\",\"Jim Meyer\",\"Scott E. Coull\",\"Alina Oprea\"],\"year\":2021,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification.\",\"authors\":[\"Ahmadreza Azizi\",\"Ibrahim Asadullah Tahmid\",\"Asim Waheed\",\"Neal Mangaokar\",\"Jiameng Pu\",\"Mobin Javed\",\"Chandan K. Reddy\",\"Bimal Viswanath\"],\"year\":2021,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Poisoning Attacks to Local Differential Privacy Protocols for Key-Value Data.\",\"authors\":[\"Yongji Wu\",\"Xiaoyu Cao\",\"Jinyuan Jia\",\"Neil Zhenqiang Gong\"],\"year\":2022,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks.\",\"authors\":[\"Shawn Shan\",\"Arjun Nitin Bhagoji\",\"Haitao Zheng\",\"Ben Y. Zhao\"],\"year\":2022,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation.\",\"authors\":[\"Xudong Pan\",\"Mi Zhang\",\"Beina Sheng\",\"Jiaming Zhu\",\"Min Yang\"],\"year\":2022,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Meta-Sift: How to Sift Out a Clean Subset in the Presence of Data Poisoning?\",\"authors\":[\"Yi Zeng\",\"Minzhou Pan\",\"Himanshu Jahagirdar\",\"Ming Jin\",\"Lingjuan Lyu\",\"Ruoxi Jia\"],\"year\":2023,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"PORE: Provably Robust Recommender Systems against Data Poisoning Attacks.\",\"authors\":[\"Jinyuan Jia\",\"Yupei Liu\",\"Yuepeng Hu\",\"Neil Zhenqiang Gong\"],\"year\":2023,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks.\",\"authors\":[\"Hamid Mozaffari\",\"Virat Shejwalkar\",\"Amir Houmansadr\"],\"year\":2023,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation.\",\"authors\":[\"Xiaoguang Li\",\"Ninghui Li\",\"Wenhai Sun\",\"Neil Zhenqiang Gong\",\"Hui Li\"],\"year\":2023,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks.\",\"authors\":[\"Jianwen Tian\",\"Kefan Qiu\",\"Debin Gao\",\"Zhi Wang\",\"Xiaohui Kuang\",\"Gang Zhao\"],\"year\":2023,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Aliasing Backdoor Attacks on Pre-trained Models.\",\"authors\":[\"Cheng'an Wei\",\"Yeonjoon Lee\",\"Kai Chen\",\"Guozhu Meng\",\"Peizhuo Lv\"],\"year\":2023,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"VILLAIN: Backdoor Attacks Against Vertical Split Learning.\",\"authors\":[\"Yijie Bai\",\"Yanjiao Chen\",\"Hanlei Zhang\",\"Wenyuan Xu\",\"Haiqin Weng\",\"Dou Goodman\"],\"year\":2023,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection.\",\"authors\":[\"Shenao Yan\",\"Shen Wang\",\"Yue Duan\",\"Hanbin Hong\",\"Kiho Lee\",\"Doowon Kim\",\"Yuan Hong\"],\"year\":2024,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Instruction Backdoor Attacks Against Customized LLMs.\",\"authors\":[\"Rui Zhang\",\"Hongwei Li\",\"Rui Wen\",\"Wenbo Jiang\",\"Yuan Zhang\",\"Michael Backes\",\"Yun Shen\",\"Yang Zhang\"],\"year\":2024,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"On the Difficulty of Defending Contrastive Learning against Backdoor Attacks.\",\"authors\":[\"Changjiang Li\",\"Ren Pang\",\"Bochuan Cao\",\"Zhaohan Xi\",\"Jinghui Chen\",\"Shouling Ji\",\"Ting Wang\"],\"year\":2024,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning.\",\"authors\":[\"Xiaoting Lyu\",\"Yufei Han\",\"Wei Wang\",\"Jingkai Liu\",\"Yongsheng Zhu\",\"Guangquan Xu\",\"Jiqiang Liu\",\"Xiangliang Zhang\"],\"year\":2024,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning.\",\"authors\":[\"Zhangchen Xu\",\"Fengqing Jiang\",\"Luyao Niu\",\"Jinyuan Jia\",\"Bo Li\",\"Radha Poovendran\"],\"year\":2024,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"UBA-Inf: Unlearning Activated Backdoor Attack with Influence-Driven Camouflage.\",\"authors\":[\"Zirui Huang\",\"Yunlong Mao\",\"Sheng Zhong\"],\"year\":2024,\"proceedings\":\"uss\",\"type\":\"attack\"},{\"title\":\"Latent Backdoor Attacks on Deep Neural Networks.\",\"authors\":[\"Yuanshun Yao\",\"Huiying Li\",\"Haitao Zheng\",\"Ben Y. Zhao\"],\"year\":2019,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features.\",\"authors\":[\"Junyu Lin\",\"Lei Xu\",\"Yingqi Liu\",\"Xiangyu Zhang\"],\"year\":2020,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"DNS Cache Poisoning Attack Reloaded: Revolutions with Side Channels.\",\"authors\":[\"Keyu Man\",\"Zhiyun Qian\",\"Zhongjie Wang\",\"Xiaofeng Zheng\",\"Youjun Huang\",\"Haixin Duan\"],\"year\":2020,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Subpopulation Data Poisoning Attacks.\",\"authors\":[\"Matthew Jagielski\",\"Giorgio Severi\",\"Niklas Pousette Harger\",\"Alina Oprea\"],\"year\":2021,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"DNS Cache Poisoning Attack: Resurrections with Side Channels.\",\"authors\":[\"Keyu Man\",\"Xin'an Zhou\",\"Zhiyun Qian\"],\"year\":2021,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets.\",\"authors\":[\"Gorka Abad\",\"Oguzhan Ersoy\",\"Stjepan Picek\",\"V\\u00edctor Julio Ram\\u00edrez-Dur\\u00e1n\",\"Aitor Urbieta\"],\"year\":2022,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Poster: Clean-label Backdoor Attack on Graph Neural Networks.\",\"authors\":[\"Jing Xu\",\"Stjepan Picek\"],\"year\":2022,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information.\",\"authors\":[\"Yi Zeng\",\"Minzhou Pan\",\"Hoang Anh Just\",\"Lingjuan Lyu\",\"Meikang Qiu\",\"Ruoxi Jia\"],\"year\":2023,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks.\",\"authors\":[\"Chulin Xie\",\"Yunhui Long\",\"Pin-Yu Chen\",\"Qinbin Li\",\"Sanmi Koyejo\",\"Bo Li\"],\"year\":2023,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Poster: RPAL-Recovering Malware Classifiers from Data Poisoning using Active Learning.\",\"authors\":[\"Shae McFadden\",\"Zeliang Kan\",\"Lorenzo Cavallaro\",\"Fabio Pierazzi\"],\"year\":2023,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Poster: Multi-target & Multi-trigger Backdoor Attacks on Graph Neural Networks.\",\"authors\":[\"Jing Xu\",\"Stjepan Picek\"],\"year\":2023,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Poster: Backdoor Attack on Extreme Learning Machines.\",\"authors\":[\"Behrad Tajalli\",\"Gorka Abad\",\"Stjepan Picek\"],\"year\":2023,\"proceedings\":\"ccs\",\"type\":\"attack\"},{\"title\":\"Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning.\",\"authors\":[\"Matthew Jagielski\",\"Alina Oprea\",\"Battista Biggio\",\"Chang Liu\",\"Cristina Nita-Rotaru\",\"Bo Li\"],\"year\":2018,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks.\",\"authors\":[\"Bolun Wang\",\"Yuanshun Yao\",\"Shawn Shan\",\"Huiying Li\",\"Bimal Viswanath\",\"Haitao Zheng\",\"Ben Y. Zhao\"],\"year\":2019,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning.\",\"authors\":[\"Virat Shejwalkar\",\"Amir Houmansadr\",\"Peter Kairouz\",\"Daniel Ramage\"],\"year\":2022,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning.\",\"authors\":[\"Jinyuan Jia\",\"Yupei Liu\",\"Neil Zhenqiang Gong\"],\"year\":2022,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers.\",\"authors\":[\"Limin Yang\",\"Zhi Chen\",\"Jacopo Cortellazzi\",\"Feargus Pendlebury\",\"Kevin Tu\",\"Fabio Pierazzi\",\"Lorenzo Cavallaro\",\"Gang Wang\"],\"year\":2023,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"BayBFed: Bayesian Backdoor Defense for Federated Learning.\",\"authors\":[\"Kavita Kumari\",\"Phillip Rieger\",\"Hossein Fereidooni\",\"Murtuza Jadliwala\",\"Ahmad-Reza Sadeghi\"],\"year\":2023,\"proceedings\":\"sp\",\"type\":\"defense\"},{\"title\":\"RAB: Provable Robustness Against Backdoor Attacks.\",\"authors\":[\"Maurice Weber\",\"Xiaojun Xu\",\"Bojan Karlas\",\"Ce Zhang\",\"Bo Li\"],\"year\":2023,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information.\",\"authors\":[\"Xiaoyu Cao\",\"Jinyuan Jia\",\"Zaixi Zhang\",\"Neil Zhenqiang Gong\"],\"year\":2023,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems.\",\"authors\":[\"Wei Zong\",\"Yang-Wai Chow\",\"Willy Susilo\",\"Kien Do\",\"Svetha Venkatesh\"],\"year\":2023,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning.\",\"authors\":[\"Haoyang Li\",\"Qingqing Ye\",\"Haibo Hu\",\"Jin Li\",\"Leixia Wang\",\"Chengfang Fang\",\"Jie Shi\"],\"year\":2023,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models.\",\"authors\":[\"Shawn Shan\",\"Wenxin Ding\",\"Josephine Passananti\",\"Stanley Wu\",\"Haitao Zheng\",\"Ben Y. Zhao\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Need for Speed: Taming Backdoor Attacks with Speed and Precision.\",\"authors\":[\"Zhuo Ma\",\"Yilong Yang\",\"Yang Liu\",\"Tong Yang\",\"Xinjing Liu\",\"Teng Li\",\"Zhan Qin\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Test-Time Poisoning Attacks Against Test-Time Adaptation Models.\",\"authors\":[\"Tianshuo Cong\",\"Xinlei He\",\"Yun Shen\",\"Yang Zhang\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge.\",\"authors\":[\"Jiahe Lan\",\"Jie Wang\",\"Baochen Yan\",\"Zheng Yan\",\"Elisa Bertino\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic.\",\"authors\":[\"Hang Wang\",\"Zhen Xiang\",\"David J. Miller\",\"George Kesidis\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"BadVFL: Backdoor Attacks in Vertical Federated Learning.\",\"authors\":[\"Mohammad Naseri\",\"Yufei Han\",\"Emiliano De Cristofaro\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Distribution Preserving Backdoor Attack in Self-supervised Learning.\",\"authors\":[\"Guanhong Tao\",\"Zhenting Wang\",\"Shiwei Feng\",\"Guangyu Shen\",\"Shiqing Ma\",\"Xiangyu Zhang\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"Exploring the Orthogonality and Linearity of Backdoor Attacks.\",\"authors\":[\"Kaiyuan Zhang\",\"Siyuan Cheng\",\"Guangyu Shen\",\"Guanhong Tao\",\"Shengwei An\",\"Anuran Makur\",\"Shiqing Ma\",\"Xiangyu Zhang\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting.\",\"authors\":[\"Huming Qiu\",\"Junjie Sun\",\"Mi Zhang\",\"Xudong Pan\",\"Min Yang\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks.\",\"authors\":[\"Ehsanul Kabir\",\"Zeyu Song\",\"Md. Rafi Ur Rashid\",\"Shagufta Mehnaz\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"},{\"title\":\"SHERPA: Explainable Robust Algorithms for Privacy-Preserved Federated Learning in Future Networks to Defend Against Data Poisoning Attacks.\",\"authors\":[\"Chamara Sandeepa\",\"Bartlomiej Siniarski\",\"Shen Wang\",\"Madhusanka Liyanage\"],\"year\":2024,\"proceedings\":\"sp\",\"type\":\"attack\"}]"
