[
    {
        "title": "Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning.",
        "authors": [
            "Virat Shejwalkar",
            "Amir Houmansadr"
        ],
        "year": 2021,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Attacks to Deep Learning Based Recommender Systems.",
        "authors": [
            "Hai Huang",
            "Jiaming Mu",
            "Neil Zhenqiang Gong",
            "Qi Li",
            "Bin Liu",
            "Mingwei Xu"
        ],
        "year": 2021,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection.",
        "authors": [
            "Phillip Rieger",
            "Thien Duc Nguyen",
            "Markus Miettinen",
            "Ahmad-Reza Sadeghi"
        ],
        "year": 2022,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks.",
        "authors": [
            "Xueluan Gong",
            "Yanjiao Chen",
            "Jianshuo Dong",
            "Qian Wang"
        ],
        "year": 2022,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "Backdoor Attacks Against Dataset Distillation.",
        "authors": [
            "Yugeng Liu",
            "Zheng Li",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "year": 2023,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "Securing Federated Sensitive Topic Classification against Poisoning Attacks.",
        "authors": [
            "Tianyue Chu",
            "\u00c1lvaro Garc\u00eda-Recuero",
            "Costas Iordanou",
            "Georgios Smaragdakis",
            "Nikolaos Laoutaris"
        ],
        "year": 2023,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense.",
        "authors": [
            "Siyuan Cheng",
            "Guanhong Tao",
            "Yingqi Liu",
            "Shengwei An",
            "Xiangzhe Xu",
            "Shiwei Feng",
            "Guangyu Shen",
            "Kaiyuan Zhang",
            "Qiuling Xu",
            "Shiqing Ma",
            "Xiangyu Zhang"
        ],
        "year": 2023,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "Automatic Adversarial Adaption for Stealthy Poisoning Attacks in Federated Learning.",
        "authors": [
            "Torsten Krau\u00df",
            "Jan K\u00f6nig",
            "Alexandra Dmitrienko",
            "Christian Kanzow"
        ],
        "year": 2024,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning.",
        "authors": [
            "Hossein Fereidooni",
            "Alessandro Pegoraro",
            "Phillip Rieger",
            "Alexandra Dmitrienko",
            "Ahmad-Reza Sadeghi"
        ],
        "year": 2024,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering.",
        "authors": [
            "Rui Zhu",
            "Di Tang",
            "Siyuan Tang",
            "Zihao Wang",
            "Guanhong Tao",
            "Shiqing Ma",
            "XiaoFeng Wang",
            "Haixu Tang"
        ],
        "year": 2024,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data.",
        "authors": [
            "Gorka Abad",
            "Oguzhan Ersoy",
            "Stjepan Picek",
            "Aitor Urbieta"
        ],
        "year": 2024,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification.",
        "authors": [
            "Hengzhi Pei",
            "Jinyuan Jia",
            "Wenbo Guo",
            "Bo Li",
            "Dawn Song"
        ],
        "year": 2024,
        "proceedings": "NDSS",
        "type": "attack"
    },
    {
        "title": "When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks.",
        "authors": [
            "Octavian Suciu",
            "Radu Marginean",
            "Yigitcan Kaya",
            "Hal Daum\u00e9 III",
            "Tudor Dumitras"
        ],
        "year": 2018,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks.",
        "authors": [
            "Ambra Demontis",
            "Marco Melis",
            "Maura Pintor",
            "Matthew Jagielski",
            "Battista Biggio",
            "Alina Oprea",
            "Cristina Nita-Rotaru",
            "Fabio Roli"
        ],
        "year": 2019,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Poison Over Troubled Forwarders: A Cache Poisoning Attack Targeting DNS Forwarding Devices.",
        "authors": [
            "Xiaofeng Zheng",
            "Chaoyi Lu",
            "Jian Peng",
            "Qiushi Yang",
            "Dongjie Zhou",
            "Baojun Liu",
            "Keyu Man",
            "Shuang Hao",
            "Haixin Duan",
            "Zhiyun Qian"
        ],
        "year": 2020,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning.",
        "authors": [
            "Minghong Fang",
            "Xiaoyu Cao",
            "Jinyuan Jia",
            "Neil Zhenqiang Gong"
        ],
        "year": 2020,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Attacks to Local Differential Privacy Protocols.",
        "authors": [
            "Xiaoyu Cao",
            "Jinyuan Jia",
            "Neil Zhenqiang Gong"
        ],
        "year": 2021,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers.",
        "authors": [
            "Giorgio Severi",
            "Jim Meyer",
            "Scott E. Coull",
            "Alina Oprea"
        ],
        "year": 2021,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification.",
        "authors": [
            "Ahmadreza Azizi",
            "Ibrahim Asadullah Tahmid",
            "Asim Waheed",
            "Neal Mangaokar",
            "Jiameng Pu",
            "Mobin Javed",
            "Chandan K. Reddy",
            "Bimal Viswanath"
        ],
        "year": 2021,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Poisoning Attacks to Local Differential Privacy Protocols for Key-Value Data.",
        "authors": [
            "Yongji Wu",
            "Xiaoyu Cao",
            "Jinyuan Jia",
            "Neil Zhenqiang Gong"
        ],
        "year": 2022,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks.",
        "authors": [
            "Shawn Shan",
            "Arjun Nitin Bhagoji",
            "Haitao Zheng",
            "Ben Y. Zhao"
        ],
        "year": 2022,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation.",
        "authors": [
            "Xudong Pan",
            "Mi Zhang",
            "Beina Sheng",
            "Jiaming Zhu",
            "Min Yang"
        ],
        "year": 2022,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Meta-Sift: How to Sift Out a Clean Subset in the Presence of Data Poisoning?",
        "authors": [
            "Yi Zeng",
            "Minzhou Pan",
            "Himanshu Jahagirdar",
            "Ming Jin",
            "Lingjuan Lyu",
            "Ruoxi Jia"
        ],
        "year": 2023,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "PORE: Provably Robust Recommender Systems against Data Poisoning Attacks.",
        "authors": [
            "Jinyuan Jia",
            "Yupei Liu",
            "Yuepeng Hu",
            "Neil Zhenqiang Gong"
        ],
        "year": 2023,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks.",
        "authors": [
            "Hamid Mozaffari",
            "Virat Shejwalkar",
            "Amir Houmansadr"
        ],
        "year": 2023,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation.",
        "authors": [
            "Xiaoguang Li",
            "Ninghui Li",
            "Wenhai Sun",
            "Neil Zhenqiang Gong",
            "Hui Li"
        ],
        "year": 2023,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks.",
        "authors": [
            "Jianwen Tian",
            "Kefan Qiu",
            "Debin Gao",
            "Zhi Wang",
            "Xiaohui Kuang",
            "Gang Zhao"
        ],
        "year": 2023,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Aliasing Backdoor Attacks on Pre-trained Models.",
        "authors": [
            "Cheng'an Wei",
            "Yeonjoon Lee",
            "Kai Chen",
            "Guozhu Meng",
            "Peizhuo Lv"
        ],
        "year": 2023,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "VILLAIN: Backdoor Attacks Against Vertical Split Learning.",
        "authors": [
            "Yijie Bai",
            "Yanjiao Chen",
            "Hanlei Zhang",
            "Wenyuan Xu",
            "Haiqin Weng",
            "Dou Goodman"
        ],
        "year": 2023,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection.",
        "authors": [
            "Shenao Yan",
            "Shen Wang",
            "Yue Duan",
            "Hanbin Hong",
            "Kiho Lee",
            "Doowon Kim",
            "Yuan Hong"
        ],
        "year": 2024,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Instruction Backdoor Attacks Against Customized LLMs.",
        "authors": [
            "Rui Zhang",
            "Hongwei Li",
            "Rui Wen",
            "Wenbo Jiang",
            "Yuan Zhang",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "year": 2024,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "On the Difficulty of Defending Contrastive Learning against Backdoor Attacks.",
        "authors": [
            "Changjiang Li",
            "Ren Pang",
            "Bochuan Cao",
            "Zhaohan Xi",
            "Jinghui Chen",
            "Shouling Ji",
            "Ting Wang"
        ],
        "year": 2024,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning.",
        "authors": [
            "Xiaoting Lyu",
            "Yufei Han",
            "Wei Wang",
            "Jingkai Liu",
            "Yongsheng Zhu",
            "Guangquan Xu",
            "Jiqiang Liu",
            "Xiangliang Zhang"
        ],
        "year": 2024,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning.",
        "authors": [
            "Zhangchen Xu",
            "Fengqing Jiang",
            "Luyao Niu",
            "Jinyuan Jia",
            "Bo Li",
            "Radha Poovendran"
        ],
        "year": 2024,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "UBA-Inf: Unlearning Activated Backdoor Attack with Influence-Driven Camouflage.",
        "authors": [
            "Zirui Huang",
            "Yunlong Mao",
            "Sheng Zhong"
        ],
        "year": 2024,
        "proceedings": "Usenix",
        "type": "attack"
    },
    {
        "title": "Latent Backdoor Attacks on Deep Neural Networks.",
        "authors": [
            "Yuanshun Yao",
            "Huiying Li",
            "Haitao Zheng",
            "Ben Y. Zhao"
        ],
        "year": 2019,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features.",
        "authors": [
            "Junyu Lin",
            "Lei Xu",
            "Yingqi Liu",
            "Xiangyu Zhang"
        ],
        "year": 2020,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "DNS Cache Poisoning Attack Reloaded: Revolutions with Side Channels.",
        "authors": [
            "Keyu Man",
            "Zhiyun Qian",
            "Zhongjie Wang",
            "Xiaofeng Zheng",
            "Youjun Huang",
            "Haixin Duan"
        ],
        "year": 2020,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Subpopulation Data Poisoning Attacks.",
        "authors": [
            "Matthew Jagielski",
            "Giorgio Severi",
            "Niklas Pousette Harger",
            "Alina Oprea"
        ],
        "year": 2021,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "DNS Cache Poisoning Attack: Resurrections with Side Channels.",
        "authors": [
            "Keyu Man",
            "Xin'an Zhou",
            "Zhiyun Qian"
        ],
        "year": 2021,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets.",
        "authors": [
            "Gorka Abad",
            "Oguzhan Ersoy",
            "Stjepan Picek",
            "V\u00edctor Julio Ram\u00edrez-Dur\u00e1n",
            "Aitor Urbieta"
        ],
        "year": 2022,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Poster: Clean-label Backdoor Attack on Graph Neural Networks.",
        "authors": [
            "Jing Xu",
            "Stjepan Picek"
        ],
        "year": 2022,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information.",
        "authors": [
            "Yi Zeng",
            "Minzhou Pan",
            "Hoang Anh Just",
            "Lingjuan Lyu",
            "Meikang Qiu",
            "Ruoxi Jia"
        ],
        "year": 2023,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks.",
        "authors": [
            "Chulin Xie",
            "Yunhui Long",
            "Pin-Yu Chen",
            "Qinbin Li",
            "Sanmi Koyejo",
            "Bo Li"
        ],
        "year": 2023,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Poster: RPAL-Recovering Malware Classifiers from Data Poisoning using Active Learning.",
        "authors": [
            "Shae McFadden",
            "Zeliang Kan",
            "Lorenzo Cavallaro",
            "Fabio Pierazzi"
        ],
        "year": 2023,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Poster: Multi-target & Multi-trigger Backdoor Attacks on Graph Neural Networks.",
        "authors": [
            "Jing Xu",
            "Stjepan Picek"
        ],
        "year": 2023,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Poster: Backdoor Attack on Extreme Learning Machines.",
        "authors": [
            "Behrad Tajalli",
            "Gorka Abad",
            "Stjepan Picek"
        ],
        "year": 2023,
        "proceedings": "CCS",
        "type": "attack"
    },
    {
        "title": "Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning.",
        "authors": [
            "Matthew Jagielski",
            "Alina Oprea",
            "Battista Biggio",
            "Chang Liu",
            "Cristina Nita-Rotaru",
            "Bo Li"
        ],
        "year": 2018,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks.",
        "authors": [
            "Bolun Wang",
            "Yuanshun Yao",
            "Shawn Shan",
            "Huiying Li",
            "Bimal Viswanath",
            "Haitao Zheng",
            "Ben Y. Zhao"
        ],
        "year": 2019,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning.",
        "authors": [
            "Virat Shejwalkar",
            "Amir Houmansadr",
            "Peter Kairouz",
            "Daniel Ramage"
        ],
        "year": 2022,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning.",
        "authors": [
            "Jinyuan Jia",
            "Yupei Liu",
            "Neil Zhenqiang Gong"
        ],
        "year": 2022,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers.",
        "authors": [
            "Limin Yang",
            "Zhi Chen",
            "Jacopo Cortellazzi",
            "Feargus Pendlebury",
            "Kevin Tu",
            "Fabio Pierazzi",
            "Lorenzo Cavallaro",
            "Gang Wang"
        ],
        "year": 2023,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "BayBFed: Bayesian Backdoor Defense for Federated Learning.",
        "authors": [
            "Kavita Kumari",
            "Phillip Rieger",
            "Hossein Fereidooni",
            "Murtuza Jadliwala",
            "Ahmad-Reza Sadeghi"
        ],
        "year": 2023,
        "proceedings": "S&P",
        "type": "defense"
    },
    {
        "title": "RAB: Provable Robustness Against Backdoor Attacks.",
        "authors": [
            "Maurice Weber",
            "Xiaojun Xu",
            "Bojan Karlas",
            "Ce Zhang",
            "Bo Li"
        ],
        "year": 2023,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information.",
        "authors": [
            "Xiaoyu Cao",
            "Jinyuan Jia",
            "Zaixi Zhang",
            "Neil Zhenqiang Gong"
        ],
        "year": 2023,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems.",
        "authors": [
            "Wei Zong",
            "Yang-Wai Chow",
            "Willy Susilo",
            "Kien Do",
            "Svetha Venkatesh"
        ],
        "year": 2023,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning.",
        "authors": [
            "Haoyang Li",
            "Qingqing Ye",
            "Haibo Hu",
            "Jin Li",
            "Leixia Wang",
            "Chengfang Fang",
            "Jie Shi"
        ],
        "year": 2023,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models.",
        "authors": [
            "Shawn Shan",
            "Wenxin Ding",
            "Josephine Passananti",
            "Stanley Wu",
            "Haitao Zheng",
            "Ben Y. Zhao"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Need for Speed: Taming Backdoor Attacks with Speed and Precision.",
        "authors": [
            "Zhuo Ma",
            "Yilong Yang",
            "Yang Liu",
            "Tong Yang",
            "Xinjing Liu",
            "Teng Li",
            "Zhan Qin"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models.",
        "authors": [
            "Tianshuo Cong",
            "Xinlei He",
            "Yun Shen",
            "Yang Zhang"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge.",
        "authors": [
            "Jiahe Lan",
            "Jie Wang",
            "Baochen Yan",
            "Zheng Yan",
            "Elisa Bertino"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic.",
        "authors": [
            "Hang Wang",
            "Zhen Xiang",
            "David J. Miller",
            "George Kesidis"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning.",
        "authors": [
            "Mohammad Naseri",
            "Yufei Han",
            "Emiliano De Cristofaro"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Distribution Preserving Backdoor Attack in Self-supervised Learning.",
        "authors": [
            "Guanhong Tao",
            "Zhenting Wang",
            "Shiwei Feng",
            "Guangyu Shen",
            "Shiqing Ma",
            "Xiangyu Zhang"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Exploring the Orthogonality and Linearity of Backdoor Attacks.",
        "authors": [
            "Kaiyuan Zhang",
            "Siyuan Cheng",
            "Guangyu Shen",
            "Guanhong Tao",
            "Shengwei An",
            "Anuran Makur",
            "Shiqing Ma",
            "Xiangyu Zhang"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting.",
        "authors": [
            "Huming Qiu",
            "Junjie Sun",
            "Mi Zhang",
            "Xudong Pan",
            "Min Yang"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks.",
        "authors": [
            "Ehsanul Kabir",
            "Zeyu Song",
            "Md. Rafi Ur Rashid",
            "Shagufta Mehnaz"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "SHERPA: Explainable Robust Algorithms for Privacy-Preserved Federated Learning in Future Networks to Defend Against Data Poisoning Attacks.",
        "authors": [
            "Chamara Sandeepa",
            "Bartlomiej Siniarski",
            "Shen Wang",
            "Madhusanka Liyanage"
        ],
        "year": 2024,
        "proceedings": "S&P",
        "type": "attack"
    },
    {
        "title": "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs.",
        "authors": [
            "Soheil Kolouri",
            "Aniruddha Saha",
            "Hamed Pirsiavash",
            "Heiko Hoffmann"
        ],
        "year": 2020,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Clean-Label Backdoor Attacks on Video Recognition Models.",
        "authors": [
            "Shihao Zhao",
            "Xingjun Ma",
            "Xiang Zheng",
            "James Bailey",
            "Jingjing Chen",
            "Yu-Gang Jiang"
        ],
        "year": 2020,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Backdoor Attacks Against Deep Learning Systems in the Physical World.",
        "authors": [
            "Emily Wenger",
            "Josephine Passananti",
            "Arjun Nitin Bhagoji",
            "Yuanshun Yao",
            "Haitao Zheng",
            "Ben Y. Zhao"
        ],
        "year": 2021,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "How Robust Are Randomized Smoothing Based Defenses to Data Poisoning?",
        "authors": [
            "Akshay Mehra",
            "Bhavya Kailkhura",
            "Pin-Yu Chen",
            "Jihun Hamm"
        ],
        "year": 2021,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free.",
        "authors": [
            "Tianlong Chen",
            "Zhenyu Zhang",
            "Yihua Zhang",
            "Shiyu Chang",
            "Sijia Liu",
            "Zhangyang Wang"
        ],
        "year": 2022,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Backdoor Attacks on Self-Supervised Learning.",
        "authors": [
            "Aniruddha Saha",
            "Ajinkya Tejankar",
            "Soroush Abbasi Koohpayegani",
            "Hamed Pirsiavash"
        ],
        "year": 2022,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks.",
        "authors": [
            "Xiangyu Qi",
            "Tinghao Xie",
            "Ruizhe Pan",
            "Jifeng Zhu",
            "Yong Yang",
            "Kai Bu"
        ],
        "year": 2022,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Few-shot Backdoor Defense Using Shapley Estimation.",
        "authors": [
            "Jiyang Guan",
            "Zhuozhuo Tu",
            "Ran He",
            "Dacheng Tao"
        ],
        "year": 2022,
        "proceedings": "CVPR",
        "type": "defense"
    },
    {
        "title": "BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning.",
        "authors": [
            "Zhenting Wang",
            "Juan Zhai",
            "Shiqing Ma"
        ],
        "year": 2022,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints.",
        "authors": [
            "Zhendong Zhao",
            "Xiaojun Chen",
            "Yuexin Xuan",
            "Ye Dong",
            "Dakui Wang",
            "Kaitai Liang"
        ],
        "year": 2022,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis.",
        "authors": [
            "Yu Feng",
            "Benteng Ma",
            "Jing Zhang",
            "Shanshan Zhao",
            "Yong Xia",
            "Dacheng Tao"
        ],
        "year": 2022,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Backdoor Defense via Adaptively Splitting Poisoned Dataset.",
        "authors": [
            "Kuofeng Gao",
            "Yang Bai",
            "Jindong Gu",
            "Yong Yang",
            "Shu-Tao Xia"
        ],
        "year": 2023,
        "proceedings": "CVPR",
        "type": "defense"
    },
    {
        "title": "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets.",
        "authors": [
            "Weixin Chen",
            "Dawn Song",
            "Bo Li"
        ],
        "year": 2023,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Color Backdoor: A Robust Poisoning Attack in Color Space.",
        "authors": [
            "Wenbo Jiang",
            "Hongwei Li",
            "Guowen Xu",
            "Tianwei Zhang"
        ],
        "year": 2023,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Backdoor Defense via Deconfounded Representation Learning.",
        "authors": [
            "Zaixi Zhang",
            "Qi Liu",
            "Zhicai Wang",
            "Zepu Lu",
            "Qingyong Hu"
        ],
        "year": 2023,
        "proceedings": "CVPR",
        "type": "defense"
    },
    {
        "title": "Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning.",
        "authors": [
            "Ajinkya Tejankar",
            "Maziar Sanjabi",
            "Qifan Wang",
            "Sinong Wang",
            "Hamed Firooz",
            "Hamed Pirsiavash",
            "Liang Tan"
        ],
        "year": 2023,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger.",
        "authors": [
            "Yi Yu",
            "Yufei Wang",
            "Wenhan Yang",
            "Shijian Lu",
            "Yap-Peng Tan",
            "Alex C. Kot"
        ],
        "year": 2023,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "You Are Catching My Attention: Are Vision Transformers Bad Learners under Backdoor Attacks?",
        "authors": [
            "Zenghui Yuan",
            "Pan Zhou",
            "Kai Zou",
            "Yu Cheng"
        ],
        "year": 2023,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Physical Backdoor: Towards Temperature-Based Backdoor Attacks in the Physical World.",
        "authors": [
            "Wen Yin",
            "Jian Lou",
            "Pan Zhou",
            "Yulai Xie",
            "Dan Feng",
            "Yuhua Sun",
            "Tailai Zhang",
            "Lichao Sun"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving.",
        "authors": [
            "Mozhgan Pourkeshavarz",
            "Mohammad Sabokrou",
            "Amir Rasouli"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "BrainWash: A Poisoning Attack to Forget in Continual Learning.",
        "authors": [
            "Ali Abbasi",
            "Parsa Nooralinejad",
            "Hamed Pirsiavash",
            "Soheil Kolouri"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP.",
        "authors": [
            "Jiawang Bai",
            "Kuofeng Gao",
            "Shaobo Min",
            "Shu-Tao Xia",
            "Zhifeng Li",
            "Wei Liu"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Based Backdoor Attacks to Contrastive Learning.",
        "authors": [
            "Jinghuai Zhang",
            "Hongbin Liu",
            "Jinyuan Jia",
            "Neil Zhenqiang Gong"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transfomers.",
        "authors": [
            "Sheng Yang",
            "Jiawang Bai",
            "Kuofeng Gao",
            "Yong Yang",
            "Yiming Li",
            "Shu-Tao Xia"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Nearest is Not Dearest: Towards Practical Defense Against Quantization-Conditioned Backdoor Attacks.",
        "authors": [
            "Boheng Li",
            "Yishuo Cai",
            "Haowei Li",
            "Feng Xue",
            "Zhifeng Li",
            "Yiming Li"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Backdoor Defense via Test-Time Detecting and Repairing.",
        "authors": [
            "Jiyang Guan",
            "Jian Liang",
            "Ran He"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "defense"
    },
    {
        "title": "BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning.",
        "authors": [
            "Siyuan Liang",
            "Mingli Zhu",
            "Aishan Liu",
            "Baoyuan Wu",
            "Xiaochun Cao",
            "Ee-Chien Chang"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Lotus: Evasive and Resilient Backdoor Attacks through Sub-Partitioning.",
        "authors": [
            "Siyuan Cheng",
            "Guanhong Tao",
            "Yingqi Liu",
            "Guangyu Shen",
            "Shengwei An",
            "Shiwei Feng",
            "Xiangzhe Xu",
            "Kaiyuan Zhang",
            "Shiqing Ma",
            "Xiangyu Zhang"
        ],
        "year": 2024,
        "proceedings": "CVPR",
        "type": "attack"
    },
    {
        "title": "Certified Defenses for Data Poisoning Attacks.",
        "authors": [
            "Jacob Steinhardt",
            "Pang Wei Koh",
            "Percy Liang"
        ],
        "year": 2017,
        "proceedings": "NeurIPS",
        "type": "attack"
    },
    {
        "title": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks.",
        "authors": [
            "Ali Shafahi",
            "W. Ronny Huang",
            "Mahyar Najibi",
            "Octavian Suciu",
            "Christoph Studer",
            "Tudor Dumitras",
            "Tom Goldstein"
        ],
        "year": 2018,
        "proceedings": "NeurIPS",
        "type": "attack"
    },
    {
        "title": "Spectral Signatures in Backdoor Attacks.",
        "authors": [
            "Brandon Tran",
            "Jerry Li",
            "Aleksander Madry"
        ],
        "year": 2018,
        "proceedings": "NeurIPS",
        "type": "attack"
    },
    {
        "title": "A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning.",
        "authors": [
            "Xuanqing Liu",
            "Si Si",
            "Jerry Zhu",
            "Yang Li",
            "Cho-Jui Hsieh"
        ],
        "year": 2019,
        "proceedings": "NeurIPS",
        "type": "attack"
    },
    {
        "title": "DBA: Distributed Backdoor Attacks against Federated Learning.",
        "authors": [
            "Chulin Xie",
            "Keli Huang",
            "Pin-Yu Chen",
            "Bo Li"
        ],
        "year": 2020,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Robust anomaly detection and backdoor attack detection via differential privacy.",
        "authors": [
            "Min Du",
            "Ruoxi Jia",
            "Dawn Song"
        ],
        "year": 2020,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching.",
        "authors": [
            "Jonas Geiping",
            "Liam H. Fowl",
            "W. Ronny Huang",
            "Wojciech Czaja",
            "Gavin Taylor",
            "Michael Moeller",
            "Tom Goldstein"
        ],
        "year": 2021,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks.",
        "authors": [
            "Alexander Levine",
            "Soheil Feizi"
        ],
        "year": 2021,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "WaNet - Imperceptible Warping-based Backdoor Attack.",
        "authors": [
            "Tuan Anh Nguyen",
            "Anh Tuan Tran"
        ],
        "year": 2021,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks.",
        "authors": [
            "Fan Wu",
            "Linyi Li",
            "Huan Zhang",
            "Bhavya Kailkhura",
            "Krishnaram Kenthapadi",
            "Ding Zhao",
            "Bo Li"
        ],
        "year": 2022,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Won't Save You From Facial Recognition.",
        "authors": [
            "Evani Radiya-Dixit",
            "Sanghyun Hong",
            "Nicholas Carlini",
            "Florian Tram\u00e8r"
        ],
        "year": 2022,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models.",
        "authors": [
            "Kangjie Chen",
            "Yuxian Meng",
            "Xiaofei Sun",
            "Shangwei Guo",
            "Tianwei Zhang",
            "Jiwei Li",
            "Chun Fan"
        ],
        "year": 2022,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios.",
        "authors": [
            "Zhen Xiang",
            "David J. Miller",
            "George Kesidis"
        ],
        "year": 2022,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Few-Shot Backdoor Attacks on Visual Object Tracking.",
        "authors": [
            "Yiming Li",
            "Haoxiang Zhong",
            "Xingjun Ma",
            "Yong Jiang",
            "Shu-Tao Xia"
        ],
        "year": 2022,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Backdoor Defense via Decoupling the Training Process.",
        "authors": [
            "Kunzhe Huang",
            "Yiming Li",
            "Baoyuan Wu",
            "Zhan Qin",
            "Kui Ren"
        ],
        "year": 2022,
        "proceedings": "ICLR",
        "type": "defense"
    },
    {
        "title": "Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning.",
        "authors": [
            "Hao He",
            "Kaiwen Zha",
            "Dina Katabi"
        ],
        "year": 2023,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?",
        "authors": [
            "Rui Wen",
            "Zhengyu Zhao",
            "Zhuoran Liu",
            "Michael Backes",
            "Tianhao Wang",
            "Yang Zhang"
        ],
        "year": 2023,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Few-shot Backdoor Attacks via Neural Tangent Kernels.",
        "authors": [
            "Jonathan Hayase",
            "Sewoong Oh"
        ],
        "year": 2023,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Revisiting the Assumption of Latent Separability for Backdoor Defenses.",
        "authors": [
            "Xiangyu Qi",
            "Tinghao Xie",
            "Yiming Li",
            "Saeed Mahloujifar",
            "Prateek Mittal"
        ],
        "year": 2023,
        "proceedings": "ICLR",
        "type": "defense"
    },
    {
        "title": "Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks.",
        "authors": [
            "Charles Jin",
            "Melinda Sun",
            "Martin C. Rinard"
        ],
        "year": 2023,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Sharpness-Aware Data Poisoning Attack.",
        "authors": [
            "Pengfei He",
            "Han Xu",
            "Jie Ren",
            "Yingqian Cui",
            "Shenglai Zeng",
            "Hui Liu",
            "Charu C. Aggarwal",
            "Jiliang Tang"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection.",
        "authors": [
            "Jiawei Liang",
            "Siyuan Liang",
            "Aishan Liu",
            "Xiaojun Jia",
            "Junhao Kuang",
            "Xiaochun Cao"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Influencer Backdoor Attack on Semantic Segmentation.",
        "authors": [
            "Haoheng Lan",
            "Jindong Gu",
            "Philip Torr",
            "Hengshuang Zhao"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Demystifying Poisoning Backdoor Attacks from a Statistical Perspective.",
        "authors": [
            "Ganghua Wang",
            "Xun Xian",
            "Ashish Kundu",
            "Jayanth Srinivasa",
            "Xuan Bi",
            "Mingyi Hong",
            "Jie Ding"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective.",
        "authors": [
            "Ming-Yu Chung",
            "Sheng-Yen Chou",
            "Chia-Mu Yu",
            "Pin-Yu Chen",
            "Sy-Yen Kuo",
            "Tsung-Yi Ho"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Universal Backdoor Attacks.",
        "authors": [
            "Benjamin Schneider",
            "Nils Lukas",
            "Florian Kerschbaum"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios.",
        "authors": [
            "Ziqiang Li",
            "Hong Sun",
            "Pengfei Xia",
            "Heng Li",
            "Beihao Xia",
            "Yi Wu",
            "Bin Li"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Rethinking CNN's Generalization to Backdoor Attack from Frequency Domain.",
        "authors": [
            "Quanrui Rao",
            "Lin Wang",
            "Wuying Liu"
        ],
        "year": 2024,
        "proceedings": "ICLR",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Attacks on Stochastic Bandits.",
        "authors": [
            "Fang Liu",
            "Ness B. Shroff"
        ],
        "year": 2019,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Attacks in Multi-Party Learning.",
        "authors": [
            "Saeed Mahloujifar",
            "Mohammad Mahmoody",
            "Ameer Mohammed"
        ],
        "year": 2019,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Transferable Clean-Label Poisoning Attacks on Deep Neural Nets.",
        "authors": [
            "Chen Zhu",
            "W. Ronny Huang",
            "Hengduo Li",
            "Gavin Taylor",
            "Christoph Studer",
            "Tom Goldstein"
        ],
        "year": 2019,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Min-Max Optimization without Gradients: Convergence and Applications to Black-Box Evasion and Poisoning Attacks.",
        "authors": [
            "Sijia Liu",
            "Songtao Lu",
            "Xiangyi Chen",
            "Yao Feng",
            "Kaidi Xu",
            "Abdullah Al-Dujaili",
            "Mingyi Hong",
            "Una-May O'Reilly"
        ],
        "year": 2020,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Adaptive Reward-Poisoning Attacks against Reinforcement Learning.",
        "authors": [
            "Xuezhou Zhang",
            "Yuzhe Ma",
            "Adish Singla",
            "Xiaojin Zhu"
        ],
        "year": 2020,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Defense against backdoor attacks via robust covariance estimation.",
        "authors": [
            "Jonathan Hayase",
            "Weihao Kong",
            "Raghav Somani",
            "Sewoong Oh"
        ],
        "year": 2021,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks.",
        "authors": [
            "Avi Schwarzschild",
            "Micah Goldblum",
            "Arjun Gupta",
            "John P. Dickerson",
            "Tom Goldstein"
        ],
        "year": 2021,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Model-Targeted Poisoning Attacks with Provable Convergence.",
        "authors": [
            "Fnu Suya",
            "Saeed Mahloujifar",
            "Anshuman Suri",
            "David Evans",
            "Yuan Tian"
        ],
        "year": 2021,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Robust Learning for Data Poisoning Attacks.",
        "authors": [
            "Yunjuan Wang",
            "Poorya Mianjy",
            "Raman Arora"
        ],
        "year": 2021,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks.",
        "authors": [
            "Chulin Xie",
            "Minghao Chen",
            "Pin-Yu Chen",
            "Bo Li"
        ],
        "year": 2021,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "On Collective Robustness of Bagging Against Data Poisoning.",
        "authors": [
            "Ruoxin Chen",
            "Zenan Li",
            "Jie Li",
            "Junchi Yan",
            "Chentao Wu"
        ],
        "year": 2022,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "An Equivalence Between Data Poisoning and Byzantine Gradient Attacks.",
        "authors": [
            "Sadegh Farhadkhani",
            "Rachid Guerraoui",
            "L\u00ea Nguy\u00ean Hoang",
            "Oscar Villemaud"
        ],
        "year": 2022,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense.",
        "authors": [
            "Guangyu Shen",
            "Yingqi Liu",
            "Guanhong Tao",
            "Qiuling Xu",
            "Zhuo Zhang",
            "Shengwei An",
            "Shiqing Ma",
            "Xiangyu Zhang"
        ],
        "year": 2022,
        "proceedings": "ICML",
        "type": "defense"
    },
    {
        "title": "Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation.",
        "authors": [
            "Wenxiao Wang",
            "Alexander Levine",
            "Soheil Feizi"
        ],
        "year": 2022,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Not All Poisons are Created Equal: Robust Training against Data Poisoning.",
        "authors": [
            "Yu Yang",
            "Tian Yu Liu",
            "Baharan Mirzasoleiman"
        ],
        "year": 2022,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Rethinking Backdoor Attacks.",
        "authors": [
            "Alaa Khaddaj",
            "Guillaume Leclerc",
            "Aleksandar Makelov",
            "Kristian Georgiev",
            "Hadi Salman",
            "Andrew Ilyas",
            "Aleksander Madry"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Reconstructive Neuron Pruning for Backdoor Defense.",
        "authors": [
            "Yige Li",
            "Xixiang Lyu",
            "Xingjun Ma",
            "Nodens Koren",
            "Lingjuan Lyu",
            "Bo Li",
            "Yu-Gang Jiang"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "defense"
    },
    {
        "title": "Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks.",
        "authors": [
            "Yiwei Lu",
            "Gautam Kamath",
            "Yaoliang Yu"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Run-off Election: Improved Provable Defense against Data Poisoning Attacks.",
        "authors": [
            "Keivan Rezaei",
            "Kiarash Banihashem",
            "Atoosa Malemir Chegini",
            "Soheil Feizi"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Understanding Backdoor Attacks through the Adaptability Hypothesis.",
        "authors": [
            "Xun Xian",
            "Ganghua Wang",
            "Jayanth Srinivasa",
            "Ashish Kundu",
            "Xuan Bi",
            "Mingyi Hong",
            "Jie Ding"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "UMD: Unsupervised Model Detection for X2X Backdoor Attacks.",
        "authors": [
            "Zhen Xiang",
            "Zidi Xiong",
            "Bo Li"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Attacks Against Multimodal Encoders.",
        "authors": [
            "Ziqing Yang",
            "Xinlei He",
            "Zheng Li",
            "Michael Backes",
            "Mathias Humbert",
            "Pascal Berrang",
            "Yang Zhang"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Graph Contrastive Backdoor Attacks.",
        "authors": [
            "Hangfan Zhang",
            "Jinghui Chen",
            "Lu Lin",
            "Jinyuan Jia",
            "Dinghao Wu"
        ],
        "year": 2023,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline.",
        "authors": [
            "Haonan Wang",
            "Qianli Shen",
            "Yao Tong",
            "Yang Zhang",
            "Kenji Kawaguchi"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks.",
        "authors": [
            "Boqi Li",
            "Weiwei Liu"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Causality Based Front-door Defense Against Backdoor Attack on Language Models.",
        "authors": [
            "Yiran Liu",
            "Xiaoang Xu",
            "Zhiyi Hou",
            "Yang Yu"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "FedREDefense: Defending against Model Poisoning Attacks for Federated Learning using Model Update Reconstruction Error.",
        "authors": [
            "Yueqi Xie",
            "Minghong Fang",
            "Neil Zhenqiang Gong"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Attacks against Conformal Prediction.",
        "authors": [
            "Yangyi Li",
            "Aobo Chen",
            "Wei Qian",
            "Chenxu Zhao",
            "Divya Lidder",
            "Mengdi Huai"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization.",
        "authors": [
            "Xingyi Zhao",
            "Depeng Xu",
            "Shuhan Yuan"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks.",
        "authors": [
            "Wenhan Yang",
            "Jingdong Gao",
            "Baharan Mirzasoleiman"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining.",
        "authors": [
            "Yudong Gao",
            "Honglong Chen",
            "Peng Sun",
            "Zhe Li",
            "Junjian Li",
            "Huajie Shao"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "defense"
    },
    {
        "title": "Generalization Bound and New Algorithm for Clean-Label Backdoor Attack.",
        "authors": [
            "Lijia Yu",
            "Shuang Liu",
            "Yibo Miao",
            "Xiao-Shan Gao",
            "Lijun Zhang"
        ],
        "year": 2024,
        "proceedings": "ICML",
        "type": "attack"
    },
    {
        "title": "Data Poisoning Attacks on Multi-Task Relationship Learning.",
        "authors": [
            "Mengchen Zhao",
            "Bo An",
            "Yaodong Yu",
            "Sulin Liu",
            "Sinno Jialin Pan"
        ],
        "year": 2018,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure.",
        "authors": [
            "Saeed Mahloujifar",
            "Dimitrios I. Diochnos",
            "Mohammad Mahmoody"
        ],
        "year": 2019,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Hidden Trigger Backdoor Attacks.",
        "authors": [
            "Aniruddha Saha",
            "Akshayvarun Subramanya",
            "Hamed Pirsiavash"
        ],
        "year": 2020,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification.",
        "authors": [
            "Siyuan Cheng",
            "Yingqi Liu",
            "Shiqing Ma",
            "Xiangyu Zhang"
        ],
        "year": 2021,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks.",
        "authors": [
            "Jinyuan Jia",
            "Xiaoyu Cao",
            "Neil Zhenqiang Gong"
        ],
        "year": 2021,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks.",
        "authors": [
            "Uday Shankar Shanthamallu",
            "Jayaraman J. Thiagarajan",
            "Andreas Spanias"
        ],
        "year": 2021,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation.",
        "authors": [
            "Zhicong Yan",
            "Gaolei Li",
            "Yuan Tian",
            "Jun Wu",
            "Shenghong Li",
            "Mingzhe Chen",
            "H. Vincent Poor"
        ],
        "year": 2021,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Backdoor Attacks on the DNN Interpretation System.",
        "authors": [
            "Shihong Fang",
            "Anna Choromanska"
        ],
        "year": 2022,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Hard to Forget: Poisoning Attacks on Certified Machine Unlearning.",
        "authors": [
            "Neil G. Marchant",
            "Benjamin I. P. Rubinstein",
            "Scott Alfeld"
        ],
        "year": 2022,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification.",
        "authors": [
            "Anshuka Rangi",
            "Long Tran-Thanh",
            "Haifeng Xu",
            "Massimo Franceschetti"
        ],
        "year": 2022,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks.",
        "authors": [
            "Jinyuan Jia",
            "Yupei Liu",
            "Xiaoyu Cao",
            "Neil Zhenqiang Gong"
        ],
        "year": 2022,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks.",
        "authors": [
            "Rui Ning",
            "Jiang Li",
            "Chunsheng Xin",
            "Hongyi Wu",
            "Chonggang Wang"
        ],
        "year": 2022,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Defending Backdoor Attacks on Vision Transformer via Patch Processing.",
        "authors": [
            "Khoa D. Doan",
            "Yingjie Lao",
            "Peng Yang",
            "Ping Li"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Defending against Backdoor Attacks in Natural Language Generation.",
        "authors": [
            "Xiaofei Sun",
            "Xiaoya Li",
            "Yuxian Meng",
            "Xiang Ao",
            "Lingjuan Lyu",
            "Jiwei Li",
            "Tianwei Zhang"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks.",
        "authors": [
            "Shijie Liu",
            "Andrew C. Cullen",
            "Paul Montague",
            "Sarah M. Erfani",
            "Benjamin I. P. Rubinstein"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Poisoning with Cerberus: Stealthy and Colluded Backdoor Attack against Federated Learning.",
        "authors": [
            "Xiaoting Lyu",
            "Yufei Han",
            "Wei Wang",
            "Jingkai Liu",
            "Bin Wang",
            "Jiqiang Liu",
            "Xiangliang Zhang"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning.",
        "authors": [
            "Young Wu",
            "Jeremy McMahan",
            "Xiaojin Zhu",
            "Qiaomin Xie"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "DeFL: Defending against Model Poisoning Attacks in Federated Learning via Critical Learning Periods Awareness.",
        "authors": [
            "Gang Yan",
            "Hao Wang",
            "Xu Yuan",
            "Jian Li"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "On the Vulnerability of Backdoor Defenses for Federated Learning.",
        "authors": [
            "Pei Fang",
            "Jinghui Chen"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "defense"
    },
    {
        "title": "End-to-End Pipeline for Trigger Detection on Hit and Track Graphs.",
        "authors": [
            "Tingting Xuan",
            "Yimin Zhu",
            "Giorgian Borca-Tasciuc",
            "Ming Xiong Liu",
            "Yu Sun",
            "Cameron Dean",
            "Yasser Corrales Morales",
            "Zhaozhong Shi",
            "Dantong Yu"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "defense"
    },
    {
        "title": "Poisoning-Based Backdoor Attacks in Computer Vision.",
        "authors": [
            "Yiming Li"
        ],
        "year": 2023,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks.",
        "authors": [
            "Tong Wang",
            "Yuan Yao",
            "Feng Xu",
            "Miao Xu",
            "Shengwei An",
            "Ting Wang"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks.",
        "authors": [
            "Tran Huynh",
            "Dang Nguyen",
            "Tung Pham",
            "Anh Tran"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Temporal-Distributed Backdoor Attack against Video Based Action Recognition.",
        "authors": [
            "Xi Li",
            "Songhe Wang",
            "Ruiquan Huang",
            "Mahanth Gowda",
            "George Kesidis"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Progressive Poisoned Data Isolation for Training-Time Backdoor Defense.",
        "authors": [
            "Yiming Chen",
            "Haiwei Wu",
            "Jiantao Zhou"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "defense"
    },
    {
        "title": "BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning.",
        "authors": [
            "Jing Cui",
            "Yufei Han",
            "Yuzhe Ma",
            "Jianbin Jiao",
            "Junge Zhang"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Backdoor Attacks via Machine Unlearning.",
        "authors": [
            "Zihao Liu",
            "Tianhao Wang",
            "Mengdi Huai",
            "Chenglin Miao"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective.",
        "authors": [
            "Zhen Qin",
            "Feiyi Chen",
            "Chen Zhi",
            "Xueqiang Yan",
            "Shuiguang Deng"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Data Poisoning to Fake a Nash Equilibria for Markov Games.",
        "authors": [
            "Young Wu",
            "Jeremy McMahan",
            "Xiaojin Zhu",
            "Qiaomin Xie"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Chronic Poisoning: Backdoor Attack against Split Learning.",
        "authors": [
            "Fangchao Yu",
            "Bo Zeng",
            "Kai Zhao",
            "Zhi Pang",
            "Lina Wang"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Robust Nonparametric Regression under Poisoning Attack.",
        "authors": [
            "Puning Zhao",
            "Zhiguo Wan"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Conditional Backdoor Attack via JPEG Compression.",
        "authors": [
            "Qiuyu Duan",
            "Zhongyun Hua",
            "Qing Liao",
            "Yushu Zhang",
            "Leo Yu Zhang"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Does Few-Shot Learning Suffer from Backdoor Attacks?",
        "authors": [
            "Xinwei Liu",
            "Xiaojun Jia",
            "Jindong Gu",
            "Yuan Xun",
            "Siyuan Liang",
            "Xiaochun Cao"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain.",
        "authors": [
            "Linkun Fan",
            "Fazhi He",
            "Tongzhen Si",
            "Wei Tang",
            "Bing Li"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models.",
        "authors": [
            "Yihao Huang",
            "Felix Juefei-Xu",
            "Qing Guo",
            "Jie Zhang",
            "Yutong Wu",
            "Ming Hu",
            "Tianlin Li",
            "Geguang Pu",
            "Yang Liu"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning.",
        "authors": [
            "Tao Liu",
            "Yuhang Zhang",
            "Zhu Feng",
            "Zhiqin Yang",
            "Chen Xu",
            "Dapeng Man",
            "Wu Yang"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models.",
        "authors": [
            "Jiachen Zhou",
            "Peizhuo Lv",
            "Yibing Lan",
            "Guozhu Meng",
            "Kai Chen",
            "Hualong Ma"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    },
    {
        "title": "BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract).",
        "authors": [
            "Zihan Guan",
            "Mengxuan Hu",
            "Zhongliang Zhou",
            "Jielu Zhang",
            "Sheng Li",
            "Ninghao Liu"
        ],
        "year": 2024,
        "proceedings": "AAAI",
        "type": "attack"
    }
]